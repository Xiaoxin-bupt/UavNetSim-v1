基于地理信标的智能协作路由策略
摘要：
飞行自组网（FANET）作为多无人机协同通信体系，广泛应用于军事侦察、灾害响应等场景，能够显著提升任务覆盖率与执行效率，相较单一无人机系统在动态任务中具备更强的适应性与协同能力。然而，在飞行自组网中，由于无人机节点的高速移动性和拓扑频繁变化，基于拓扑的路由协议很难实时维护全局拓扑，而已知的地理路由协议则存在应用场景单一、路由空洞及路径冗余等挑战。针对上述问题，本文提出一种基于地理信标的智能协作路由策略（Geographic Beacon-based Intelligent Collaborative Routing, GBICR）。首先该策略通过地理信标更新机制，在限制信令开销的前提下感知网络概况，拓展了应用场景。其次构建多维链路效用度量模型，综合评估转发进展、链路稳定性、单跳延迟及邻居密度对路由决策的影响。最后引入基于邻居链路效用度量的智能协同路由决策机制，通过链路效用加权与局部信息交互对 Q 值计算进行优化，在提升路由协同性的同时增强了路由健壮性。仿真结果表明，所提出的路由算法在动态拓扑场景下相较现有协议显著提升了网络性能。
关键词：强化学习，自适应路由，飞行自组网
一、引言
随着低空经济蓬勃发展，无人机（Unmanned Aerial Vehicles, UAVs）在侦察监视、应急救援、森林防火等军事与民用场景中发挥着日益重要的作用 [1]。复杂任务对无人机的智能化与协同作业能力提出了更高要求，单一节点因受限于通信能力和任务持续性，难以胜任高动态环境下的任务需求。相较而言，多无人机构成的协同网络具备任务分担、实时共享与容错能力，在无地面基础设施支持下可实现高效稳定的分布式作业 [2][3]。飞行自组织网（Flying Ad hoc Networks, FANETs）作为移动自组织网络（MANET）在空域的延伸，面临更剧烈的拓扑变化与链路不稳定问题 [4]。其高动态性、链路易失性与网络稀疏性路由策略的设计带来严峻挑战，尤其对路由机制的可靠性与效率提出迫切需求。
当前FANET路由协议主要分为基于拓扑的和基于地理信息的两大类。其中，拓扑类协议又细分为主动式、反应式和混合式。主动式协议维护开销大，反应式协议存在明显路由发现延迟，而混合式协议则难以兼顾维护效率与实时性。相比之下，基于位置的地理路由虽然能在一定程度上降低路由开销，但在存在“路由空洞”，即在当前节点覆盖范围内无更接近目标节点的邻居时，常常导致路径冗余甚至数据传输失败 [5]，严重影响网络可靠性。
强化学习（Reinforcement Learning, RL）技术近年来被广泛应用于FANET中的路由决策问题 [6]。由于路由的路径选择仅依赖于当前网络状态，通常可建模为马尔可夫决策过程（MDP），而RL在求解此类问题上具备显著优势。与传统协议相比，RL具备良好的环境感知与自适应能力，能够依据环境状态不断优化路由策略，以适应高度动态的网络拓扑 [7]。
在[8]中，作者最早将Q学习用于静态场景中的路由问题。QGrid [9] 通过将网络区域网格化并结合Q值表进行转发决策；QGeo [10] 在此基础上引入数据包传输速度，在移动场景中获得更优性能。但上述方法大多采用固定学习率，难以应对FANET中频繁变化的链路状态，导致学习收敛性和适应性不足。
随后的QMR [11] 与 QLGR [12] 引入了自适应学习率与多重奖励机制，部分缓解了上述问题。然而，这些方法仍存在三方面不足：一是路由策略过于依赖单跳延迟或能耗，缺乏多维链路状态建模；二是学习过程缺乏对全局拓扑动态演化的感知，难以实现长时路径优化；三是面对如路由空洞等极端场景时，缺乏健壮的应对策略。
为此，QFAGR [13] 通过引入动态学习参数增强策略适应性，在一定程度上提升了路径健壮性。但其链路评估仍主要依赖局部视角，缺乏对全局状态的系统建模与协作意识的体现，仍难满足高动态环境下的高效路由需求。
综上所述，现有方法普遍面临两难困境：一方面，频繁更新全局信息导致信令开销激增；另一方面，局部决策易陷入次优路径选择。如何在动态网络环境中实现全局信息感知与局部决策效率之间的有效权衡，成为提升FANET路由性能的关键难点。针对上述挑战，本文提出一种基于地理信标的智能协作路由策略（Geographic Beacon-based Intelligent Collaborative Routing,GBICR），旨在构建兼顾通信效率与决策精度的健壮性路由机制。具体而言，本文的主要贡献包括：
（1）提出了地理信标机制：通过稀疏触发的全局位置梯度同步策略减少控制信令开销，同时为局部路由提供全局拓扑的先验知识。
（2）构建了多维链路效用度量模型：融合深度强化学习，综合评估转发进展、链路稳定性、单跳延迟及邻居密度对路由决策的影响，优化路由决策。
（3）设计了基于强化学习的智能协作路由机制：在不引入额外通信开销的前提下，通过加权融合链路效用度量与Q值估计实现分布式协同决策。该机制在提升路由协同性的同时，有效缓解路径冗余与路由空洞问题，增强了整体路由的健壮性与效率。



二、系统模型
在本节中，提出了一种基于Q-learning的智能协作路由协议。在时延和可移动性的情况下，节点利用局部信息与地理信标信息，根据强化学习算法自主选择最优路径。为了拓展协议的应用场景，采用地理信标机制，支持任意网络中任意节点之间的路由。针对路由空洞场景，采用特殊的路由空洞避免算法与路由恢复准则。
2.1邻居发现与全局协作
为实现高效的拓扑感知和节点协作，本协议设计了两种信息交换机制：周期性HELLO包用于本地邻居发现，地理信标（Geographic Beacon）用于全局位置信息更新。两种机制协同工作，为路由决策提供更为全面的环境信息。
（1）本地邻居发现
每个无人机节点会定期发送HELLO消息，其中包含节点标识、时间戳、节点位置、速度、节点数量、最大Q值。邻居节点在接收到来自其他节点的HELLO消息后，会建立并维护自己的邻居表。每个节点都利用邻居表的信息来感知网络状况。如果超过没有刷新邻居信息，该邻居将被从邻居表中删除。可以根据节点的移动速度调整HELLO消息和的时间间隔。节点的移动速度越快，HELLO消息和的时间间隔越小。相反，HELLO消息的时间间隔和值会变大。
（2）地理信标机制
地理信标通过多跳逐步更新转发传播节点的位置信息，拓展路由协议的应用场景。信标最初仅仅包含当前的节点的地理信息组，即节点标识、节点位置信息，节点速度 以及当前时间戳以确保信息时效性。当节点接收信标后，更新其全局位置表，并将当前节点的位置信息组添加至信标中，更新信标信息。
为减少冗余开销，节点仅向未在信标中出现过的邻居节点进行转发。而后在转发途中不断记录途径节点的地理信息组，最终到达节点的信标包含途径所有节点的地理信息组。每个节点的信标更新周期为的α倍，以平衡信息更新频率和通信开销。
2.2移动模型
在选择下一跳时，会综合考虑链路剩余时间和周边节点的链路稳定性。邻居节点的链路剩余时间计算公式如下：

其中表示节点的通信范围。分别代表节点  与邻居节点  的位置坐标。同理，则对应节点  与邻居节点  的速度值。值越大，说明邻居节点处于通信范围内的时长越长；反之则越短。为避免数据出现剧烈抖动现象，采用如下平滑处理方法：

其中表示当前时刻，链路剩余时间取最近 n 个时刻的均值。
链路稳定性的计算公式如下：

其中和分别表示节点i与相邻节点j的速度。为最大速度限制值。当值越大时，说明节点i与其相邻节点之间的相对速度越接近，链路稳定性越高；反之则越不稳定。同样采用类似公式(3)的平滑处理方法。链路移动性指数由链路剩余时间和链路稳定性共同构成，具体表达式如下：

其中 。当链路剩余时间长且稳定性高时， 也越大，反之亦然。
2.3延迟模型
考虑实时应用需求，定义单跳时延 如下：

其中是介质访问（MAC）延迟，通过ACK消息估计如下：

其中是节点从邻居节点接收ACK消息的时刻，是节点向邻居节点发送数据包的时刻。
表示排队延迟，即数据包从排队到发送的延迟，如下所示：

其中是当前数据包前面排队的数据包数量，表示每个数据包的处理时间。
为传输延迟，如下所示：

其中  为数据包大小， 为传输速率。

2.4. Q 路由模型
本文将路由过程建模为一个马尔可夫决策过程（MDP）。其路由转发决策基于数据包的当前环境状态。路由过程的MDP由元组（S，A，R，S′）构成，其中S为智能体的当前状态，A为智能体在当前状态下所采取的动作，R是表示由于采取动作 A 而获得的反馈。S′表示在状态 S 下采取动作 A 之后，系统转移到的下一个状态。
每个数据包都扮演着智能体的角色。通过从有限邻近节点中选择最优下一跳，这一过程遵循了Q学习算法在有限马尔可夫决策过程中应用的基本原理，使得该算法能够应用于路由问题。Q学习的核心目标是通过迭代学习状态-动作对的值，从而获得最优策略。Q值更新公式如下所示：

其中表示在状态s下采取动作a的Q值，是执行动作a的奖励值，代表未来Q值期望值，即未来状态中的最大Q值，是学习率，是折扣因子。可以看出，Q值的更新既依赖于先前Q值的累积，也涉及未来Q值期望值的考量。为适应FANETs网络的动态拓扑结构，将采用连续在线学习策略。
同时，为了加快Q值迭代速度，每当一个数据包发送时，除了学习当前选择的动作，还会同时学习那些未被选择但比当前节点更接近目的节点的邻居。这样可以提前获得不同转发路径的信息。更新公式如下：

其中：为满足条件的邻居集合，为集合中的节点，为目的节点，由于这些动作并未被实际选择，其奖励值会衰减系数打折扣。
2.5. 路由算法设计
无人机节点通过HELLO消息发现邻近节点。为了将数据包传输至更接近目标节点的位置，系统会优先选择比当前节点更靠近目标节点的邻近节点作为候选邻居。在传输数据包时，若候选邻居集合为空，则会执行路由孔洞规避机制；否则，根据候选邻居集合中的最大Q值确定下一跳转发节点，按照奖励函数获取该操作对应的奖励，并更新Q值。
（1） 状态空间建模
目前已有的飞行自组网路由算法往往将目的节点设为固定位置的信号基站，应用场景有限。而本算法中支持每个节点均具备独立的无线通信与移动能力，且具备同等地位，既可作为数据源节点，也可充当中继或目标节点。
因此，智能体的每个状态包括数据包的当前所在节点id以及数据包的目的节点id。例如，若数据包当前位于节点，且目的节点为节点，则其状态S为，假设转发到邻居节点，则状态转移后的S′为。
（2） 动作空间建模
在每个决策周期，智能体从邻居集合中选择一个节点作为数据包的下一跳。动作空间定义为：

其中，通过HELLO包维护，和基于地理信标更新的位置信息计算。该约束确保选择进展性邻居，避免路由环路。若，表明节点陷入路由空洞，触发后续的空洞规避机制。
（3） 奖励函数设计
奖励函数引导智能体学习高效路由策略，综合考虑转发进展、延迟和链路寿命、邻居数量多维因素。节点在状态执行动作后的即时奖励定义为：


其中，其中， 表示目的节点方向上的距离进展， 为单跳传输时延， 为链路预计维持时间， 为节点的邻居数量， 为各特征的权重，且 。所有特征均归一化至  区间，以保证可比性。
激励最短路径，奖励正确转发到邻居节点，规避无效决策。即时奖励的计算是由下一跳节点收到数据包之后根据实际传输的情况，并放参数信息到数据包的ACK报文中反馈给节点来计算，若节点在最大ACK等待时间之内没有收到该路由包的ACK报文，便认定该路由包发送失败。奖励函数通过正负反馈和协作激励，为空洞规避和协作机制提供支持。
（4）路由空洞避免
当节点 j 在路由决策过程中遇到路由空洞时，它会向一跳邻居广播 HOLE 消息。邻居节点在接收到 HOLE 消息后，会将目的地为d从自身到 j 的链路奖励设为最小值 ，并更新对应的 Q 值。这样邻居节点能够识别 j 遇到了路由空洞，并避免再次选择它。
对于节点 j 自身的数据包，会采用新的路由恢复准则：优先选择节点度较大且距离目的节点更近的邻居作为下一跳。节点度大意味着拥有更多邻居，遇到空洞的概率较小；同时更接近目的节点则提高了路由效率。
评分公式如下：

其中：为当前节点，为邻居节点，为目的节点，为通信范围， 为节点的邻居数量。选择得分最高的邻居作为新的下一跳，以帮助数据包成功跳出路由空洞。


三、智能协作路由决策
3.1链路效用度量
在高动态的飞行自组网环境中，仅依赖单一因素（如几何距离或单跳时延）难以全面反映链路质量。为此，本文设计了一个多维链路效用度量模型，用于刻画候选邻居在稳定性与传输效率方面的综合表现。具体而言，对于当前节点  与候选邻居 ，其链路效用定义为：

其中，各特征项的含义与公式（13）中的一致，所不同的是奖励函数的各个特征项的来源是由数据包实际到达下一跳节点之后的真实传输信息，而链路效用度量是在发送数据包之前根据本地节点的邻居信息表所预估的。链路效用度量作为即时链路质量的量化结果，将在后续的动作选择过程中与 Q 值融合，从而兼顾历史经验与实时状态，提升路由决策的适应性。
3.2 动作选择机制
基于前文定义的链路效用度量，本文进一步提出融合打分机制来指导动作选择。在状态  下，节点对候选邻居  的决策分数定义为：

其中， 反映了节点基于历史交互所学习到的经验知识，而  则提供了对当前链路状态的即时刻画。
自适应权重系数  定义为：


其中，，分别表示自适应权重系数的最小值与最大值，  为节点 i 在时刻  与 t 的邻居集合。自适应权重系数  随网络拓扑动态性自适应调整，在邻居集合频繁更替或 ACK 波动较大时，增大 ，使决策更多依赖实时链路效用；当网络较为稳定时，减小 ，以充分发挥 Q 值的长期学习优势。最终，节点采用 -greedy 策略在最高分邻居与其他邻居之间进行选择，实现探索与利用的平衡。
3.3自适应参数
	传统协议中多采用固定学习率。但 FANETs 链路不稳定，固定学习率可能导致过时信息影响结果。为此，采用基于链路质量的自适应学习率。链路质量定义为：

其中和分别表示节点i在一段时间内发送的HELLO消息数量以及节点j回复给节点i的HACK消息数量。后者是相同的。的取值范围是01。
学习率取：

折扣因子反映未来 Q 值期望的稳定性。若节点在短时间内邻居变化频繁，则降低折扣因子。折扣因子定义为：

其中  为节点 i 在时刻  与 t 的邻居集合。
HELLO消息间隔的自适应调整方式如下：

其中其中是自适应折扣因子， 为时间常数。网络稳定时延长间隔以减少开销，拓扑频繁变化时则缩短间隔以保证状态更新及时。


五、仿真实验与结果分析
本章通过仿真实验验证所提出基于地理信标的智能协作路由策略（GBICR）的性能优势。实验在构建的飞行自组网仿真环境中进行，重点考察了动态拓扑下的数据包传输成功率、数据传输延迟、网络吞吐量等参数。

5.1实验设置
仿真环境中部署  个无人机节点组成，部署于一个  米的二维平面区域内。每个节点均具备独立的无线通信与移动能力，且具备同等地位，既可作为数据源节点，也可充当中继或目标节点。节点初始位置服从二维均匀分布，确保仿真初始状态不带偏差。节点移动遵循高斯-马尔可夫模型，通信半径及网络规模符合实际FANET特征。对比协议包括传统地理路由协议GPSR、强化学习路由协议QGeo和QMR等典型方案。关键参数如HELLO包周期、业务数据包生成速率等均保持一致。

Parameters	Values
Routing protocol	GBICR, QMR, QGeo, GPSR
Number of nodes	20
Simulation area	X:0-800m, Y:0-800m
MAC	IEEE 802.11p
Speed	10, 20, 30, 40, 50, 60m/s
Simulation time	100s
Mobility model	Gaussian Markov Model
Propagation loss model	RangePropagationLossModel, range=200m
Number of sinks	10
Packet size	1024bytes
Data generation rate	2048bps
Traffic type	CBR
主要评估指标包括：
数据包传输成功率：衡量路径选择过程中遇到无合适下一跳节点的频率。
平均端到端传输延迟：衡量数据包从源节点到目标节点所需平均时间；
网络吞吐量：单位时间内成功传输的数据量，反映网络传输效率；

5.2结果分析
仿真结果显示，GBICR在指标上：
（1）数据包传输成功率：


（2）平均端到端传输延迟：


（3）网络吞吐量：


综合仿真结果表明，GBICR有效缓解了传统地理路由中存在的路由空洞和路径冗余问题，显著提升了FANET的通信效率和稳定性，为多无人机协同通信提供了可靠的路由保障。

结论与展望
本文提出了一种基于地理信标的智能协作路由策略（GBICR），综合考虑转发进展、单跳延迟、链路稳定性、链路稳定性、邻居数量对路由决策的影响，实现高效可靠的路由决策。通过低频地理信标机制，限制信令开销的前提下拓展了应用场景。采用强化学习优化路由选择，结合邻居链路效用加权评分，实现分布式协作决策，有效缓解路由空洞和路径冗余问题。仿真结果表明，GBICR在动态拓扑环境下与QGeo、QMR及GPSR等现有方案相比展现出显著优势。


参考文献
1.M. Y. Arafat and S. Moh, "Routing protocols for unmanned aerial vehicle networks: A survey", IEEE access, vol. 7, pp. 99694-99720, 2019.
2.H. T. Do, H. T. Hua, M. T. Nguyen, C. V. Nguyen, H. T. Nguyen, H. T. Nguyen, et al., "Formation control algorithms for multiple-UAVs: a comprehensive survey", EAI Endorsed Transactions on Industrial Networks and Intelligent Systems, vol. 8, no. 27, 2021.
3.L. Gupta, R. Jain and G. Vaszkun, "Survey of important issues in UAV communication networks", IEEE communications surveys & tutorials, vol. 18, no. 2, pp. 1123-1152, 2015.
4. J.-D. M. M. Biomo, T. Kunz and M. St-Hilaire, "Directional antennas in FANETs: A performance analysis of routing protocols", 2017 International Conference on Selected Topics in Mobile and Wireless Networking (MoWNeT), pp. 1-8, 2017.
5. D. S. Lakew, U. Sa'ad, N.-N. Dao, W. Na and S. Cho, "Routing in flying ad hoc networks: A comprehensive survey", IEEE Communications Surveys & Tutorials, vol. 22, no. 2, pp. 1071-1120, 2020.
6. J. Lansky, S. Ali, A. M. Rahmani, M. S. Yousefpoor, E. Yousefpoor, F. Khan, et al., "Reinforcement learning-based routing protocols in flying ad hoc networks (FANET): A review", Mathematics, vol. 10, no. 16, pp. 3017-3017, 2022.
7. W. Jin, R. Gu and Y. Ji, "Reward function learning for Q-learning-based geographic routing protocol", IEEE Communications Letters, vol. 23, no. 7, pp. 1236-1239, 2019.
8.J. Boyan and M. Littman, "Packet routing in dynamically changing networks: A reinforcement learning approach", Advances in neural information processing systems, vol. 6, pp. 671-678, 1993.
8. J. Boyan and M. Littman, "Packet routing in dynamically changing networks: A reinforcement learning approach", Advances in neural information processing systems, vol. 6, pp. 671-678, 1993.
9. R. Li, F. Li, X. Li and Y. Wang, "QGrid: Q-learning based routing protocol for vehicular ad hoc networks", 2014 IEEE 33rd international performance computing and communications conference (IPCCC), pp. 1-8, 2014.
10. W.-S. Jung, J. Yim and Y.-B. Ko, "QGeo: Q-learning-based geographic ad hoc routing protocol for unmanned robotic networks", IEEE Communications Letters, vol. 21, no. 10, pp. 2258-2261, 2017.
11. J. Liu, Q. Wang, C. He, K. Jaffrès-Runser, Y. Xu, Z. Li, et al., "QMR: Q-learning based multi-objective optimization routing protocol for flying ad hoc networks", Computer Communications, vol. 150, pp. 304-316, 2020.
12. X. Qiu, Y. Xie, Y. Wang, L. Ye and Y. Yang, "QLGR: A Q-learning-based geographic FANET routing algorithm based on multi agent re-inforcement learning", KSII Transactions on Internet & Information Systems, vol. 15, no. 11, pp. 4244-4274, 2021.
13.C. Wei, Y. Wang, X. Wang and Y. Tang, "QFAGR: A Q-learning-based Fast Adaptive Geographic Routing Protocol for Flying Ad hoc Networks," GLOBECOM 2023 - 2023 IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp. 4613-4618


